{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perikleidakis_IC1180010.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGuV85fM6CmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Created on Sun Aug 23 16:55:01 2020\n",
        "\n",
        "@author: Giannis\n",
        "\n",
        "sources: \n",
        "datacamp.com/community/tutorials/principal-component-analysis-in-python\n",
        "rpubs.com/Sharon_1684/454441\n",
        "kaggle.com/olaniyan/image_classification_using_knn\n",
        "\n",
        "Θα πρέπει να γινει upload το συμπιεσμένο αρχείο των εικόνων\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "import zipfile\n",
        "import io\n",
        "data = zipfile.ZipFile(io.BytesIO(uploaded['images.zip']), 'r')\n",
        "data.extractall()\n",
        "data.printdir()\n",
        "# To decode the files\n",
        "import pickle\n",
        "# For array manipulations\n",
        "import time\n",
        "import numpy as np\n",
        "# To make one-hot vectors\n",
        "from keras.utils import np_utils\n",
        "# To plot graphs and display images\n",
        "from matplotlib import pyplot as plt\n",
        "path = \"images/\" \n",
        "\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "size = 100,100;\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "import imageio\n",
        "from PIL import Image\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import pandas as pd\n",
        "y_tr = 0\n",
        "#fig, ax = plt.subplots()\n",
        "\n",
        "# 5-fold #\n",
        "#kf = StratifiedKFold(n_splits=5, random_state=123)\n",
        "\n",
        "def convert_word_to_label(word):\n",
        "\n",
        "    if word == 'W':\n",
        "        return 0\n",
        "    elif word == 'F':\n",
        "        return 1\n",
        "    elif word == 'S':\n",
        "        return 2\n",
        "    \n",
        "\n",
        "def PCA_ImageSpaceVisualization(x_train):\n",
        "    np.min(x_train),np.max(x_train)\n",
        "    \n",
        "    x_tr = np.asarray(x_train)/255.0\n",
        "    x_tr.shape\n",
        "      \n",
        "    x_train_flat = x_tr.reshape(-1,30000)\n",
        "    print (x_train_flat.shape)\n",
        "    feat_cols = ['pixel'+str(i) for i in range(x_train_flat.shape[1])]\n",
        "    df_cifar = pd.DataFrame(x_train_flat,columns=feat_cols)\n",
        "    df_cifar['label'] = y_tr\n",
        "    #print('Size of the dataframe: {}'.format(df_cifar.shape))\n",
        "    #print(df_cifar.head())\n",
        "    pca_cifar = PCA(n_components=2)\n",
        "    principalComponents_cifar = pca_cifar.fit_transform(df_cifar.iloc[:,:-1])\n",
        "    \n",
        "    \n",
        "    principal_cifar_Df = pd.DataFrame(data = principalComponents_cifar, columns = ['principal component 1', 'principal component 2'])\n",
        "    principal_cifar_Df['y'] = y_tr\n",
        "    #print(principal_cifar_Df.head())\n",
        "    \n",
        "    plt.figure(figsize=(16,10))\n",
        "    sns.scatterplot(\n",
        "    x=\"principal component 1\", y=\"principal component 2\",\n",
        "    hue=\"y\",\n",
        "    palette=sns.color_palette(\"hls\", 3),\n",
        "    data=principal_cifar_Df,\n",
        "    legend=\"full\",\n",
        "    alpha=0.6,s=150\n",
        ")\n",
        "    \n",
        "    plt.figure(figsize=(10, 10))\n",
        "    \n",
        "\n",
        "    #linear SVM\n",
        "\n",
        "    ss = StandardScaler()\n",
        "    # run this on our feature matrix\n",
        "    x_train_flat2 = ss.fit_transform(x_train_flat)\n",
        "    \n",
        "    pca = PCA(.95)\n",
        "    # use fit_transform to run PCA on our standardized matrix\n",
        "    pca.fit_transform(x_train_flat2)\n",
        "    #print(pca.n_components_ )   \n",
        "  \n",
        "    X = pd.DataFrame(x_train_flat2)\n",
        "    y = pd.Series(y_tr)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n",
        "    svm = SVC(kernel='linear', probability=True, random_state=42)\n",
        "    knn = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=1))\n",
        "    # fit model\n",
        "    svm.fit(X_train,y_train)\n",
        "    knn.fit(X_train,y_train)\n",
        "   # generate predictions\n",
        "    \n",
        "    #y_pred = svm.predict(X_test)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    # calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print('Model accuracy for 1-nn is: ', accuracy)\n",
        "    res_svm = cross_val_score(svm, X, y, cv=5)\n",
        "    res_knn = cross_val_score(knn, X, y, cv=5)\n",
        "    \n",
        "    avg_score_svm = np.mean(res_svm)\n",
        "    avg_score_knn = np.mean(res_knn)\n",
        "    print(\"SVM k-fold\",res_svm,\"avg:\",avg_score_svm)\n",
        "    print(\"1NN k-fold\",res_knn,\"avg:\",avg_score_knn)\n",
        "    #scoring = ['accuracy'] \n",
        "          \n",
        "    return principal_cifar_Df\n",
        "\n",
        "def loadImages(path):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for r, d, f in os.walk(path):\n",
        "        for image in f:\n",
        "            if '.jpg' in image:\n",
        "                image_path = os.path.join(r, image)\n",
        "                img = cv2.imread(image_path)\n",
        "                new_img = cv2.resize(img, (size), interpolation=cv2.INTER_CUBIC)\n",
        "                X.append(new_img)\n",
        "                word = re.split('(\\d+)',image)[0]\n",
        "                y.append(convert_word_to_label(word))\n",
        "    return X, y\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    X, y = loadImages(path)\n",
        "    y_tr = y\n",
        "    a = PCA_ImageSpaceVisualization(X)\n",
        "    #print (a)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYbcBdz366bH",
        "colab_type": "text"
      },
      "source": [
        "**Απαντήσεις  στα ερωτήματα του 2**\n",
        "\n",
        "Χρησιμοποιώντας την PCA στο παράδειγμά μας καταφέραμε να εξάγουμε μια αναπαράσταση χαμηλών διαστάσεων η οποία μπορεί να χρησιμοποιηθεί στη θέση των αρχικών δεδομένων για ταξινόμηση και οπτικοποίηση (προβολή δεδομένων στο χώρο μικρότερης διάστασης)\n",
        "    \n",
        "Οι εικόνες που οπτικοποιούνται σε κοντινή απόσταση σε αυτό το χώρο δύο διαστάσεων έχουν κάποια κοινά χαρακτηριστικά, για παράδειγμα διαθέτουν παρόμοια χρώματα και μπορεί να έχουν ληφθεί την ίδια εποχή του χρόνου.\n",
        "  \n",
        "Γενικά τα δεδομένα στο χώρο των δύο διαστάσεων μπορεί να μην είναι γραμμικώς διαχωρίσιμα,όπως φαίνεται στο παράδειγμα για τις εικόνες που ανήκουν στις δύο απο τις τρείς εποχές που μελετάμε (άνοιξη, φθινόπωρο). Για να γίνει ακριβής διαχωρισμός ενδεχομένως χρειάζεται η προβολή των δεδομένων σε μεγαλύτερο χώρο διαστάσεων.\n",
        "  \n",
        "Όταν απέχουν πολύ οι εικόνες στην οπτικοποίηση αυτό σημαίνει οτι απεικονίζουν κάτι πολύ διαφορετικό και δεν θα μπορούσαν να ομαδοποιηθούν στην ίδια κατηγορία απο τον ταξινομητή. Δείχνει η απόσταση και την ύπαρξη μεγάλης διακύμανσης στα δεδομένα. Η ερμηνευόμενη διακύμαση διατηρείται στα δεδομένα με τις συνιστώσες που έχουμε διατηρήσει."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYOXVk2B6OBF",
        "colab_type": "text"
      },
      "source": [
        "**Απαντήσεις στα ερωτήματα του 3**\n",
        "   \n",
        "Mαθηματικός ορισμός της ακρίβειας (accuracy)\n",
        "Accuracy = Number of correct predictions/Total number of predictions = \n",
        "TP+TN/ TP + TN + FP + FN, όπου TP = Positive True, FP = False Positive FN = False Negative\n",
        "                            TN = True Negative\n",
        "   \n",
        "O ταξινομητής που φαίνεται να έχει την καλύτερη επίδοση με βάση τα αποτελέσματα στα πειράματα είναι ο ταξινομηνής linear SVM. Ο λόγος είναι ότι η τιμή της υπερπαραμέτρου k στον αλγόριθμο Κ-nn δέν είναι η βέλτιστη. Δηλαδή λαμβάνουμε υπόψη έναν μόνο γείτονα για την ταξινόμηση κι αυτό δεν είναι αρκετά αποδοτικό.\n",
        "    \n",
        "Mε την εντολή, για παράδειγμα pca = PCA(.95) μπορούμε να προσδιορίσουμε  \n",
        "τη διάσταση των χαρακτηριστικών που θέλουμε να εξάγουμε μέσω της PCA. Εκτυπώνοντας τον πίνακα που προκύπτει απο την επεξεργασια των δεδομένων μπορούμε να δούμε τις νεες διαστάσεις. Δηλαδή επιλέγουμε αριθμό κύριων διαστάσεων έτσι ώστε να κρατήσουμε το 95% της διακυμανσης στο παράδειγμα. Εναλλακτικά, pca = PCA(n_components=αριθμος διαστασεων)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMncdkaz7ydr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "ερωτημα 2\n",
        "@author: Giannis\n",
        "RegNMF (Regulated non matrix factorization)\n",
        "Is an alternative solution for the generalized PCA and regularized PCA. \n",
        "This keeps  the non negative numbers of the input array and has easier interpretable components.\n",
        "---Returns---\n",
        "The non negative tables W (d x k) and C (k X N) fully representing the X.\n",
        "C   :   matrix of components\n",
        "W   :   is the base\n",
        "---Inputs---\n",
        "X   :   non negative table dxN = (500 x 1000). \n",
        "        the input array for decomposition\n",
        "k   :   integer \n",
        "        the number of components\n",
        "lambda_ :   float \n",
        "            the regularization parameter\n",
        "epsilon :   float\n",
        "            the lowest threshold for the termination\n",
        "\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def gradientDescent_C(X, W, C):\n",
        "    W_transp = np.transpose(W)\n",
        "    nom = np.dot(W_transp,X)\n",
        "    denom = np.linalg.multi_dot([W_transp, W, C])\n",
        "    div = np.divide(nom, denom)\n",
        "    C = np.multiply(C, div)\n",
        "    return C\n",
        "\n",
        "\n",
        "def gradientDescent_W(X, W, C):\n",
        "    C_transp = np.transpose(C)\n",
        "    nom = np.dot(X, C_transp)\n",
        "    denom = np.linalg.multi_dot([W, C, C_transp])\n",
        "    div = np.divide(nom, denom)\n",
        "    W = np.multiply(W, div)\n",
        "    return W\n",
        "\n",
        "\n",
        "def error_(X, W, C):\n",
        "    dist = np.linalg.norm(np.subtract(X, np.dot(W, C)))\n",
        "    X_norm_2 = np.linalg.norm(X)\n",
        "    error = np.divide(dist, X_norm_2)\n",
        "    return error\n",
        "\n",
        "\n",
        "def RegNMF(X, k, lambda_, epsilon):\n",
        "    D = 500\n",
        "    N = 1000\n",
        "    iters = 500\n",
        "    # n = 0.001 # step\n",
        "    W = abs(np.random.rand(D, k))     # random initializions on W, C\n",
        "    C = abs(np.random.rand(k, N))\n",
        "    for e in epsilon:\n",
        "        for t in range(1, iters):\n",
        "            W_prev = W\n",
        "            C_prev = C\n",
        "            C = gradientDescent_C(X, W, C)\n",
        "            W = gradientDescent_W(X, W, C)\n",
        "            error_t_prev = error_(X, W_prev, C_prev)\n",
        "            error_t = error_(X, W, C)\n",
        "            # print(\"error_t : \", error_t)\n",
        "            # print(\"error_t_prev : \", error_t_prev)\n",
        "\n",
        "            if abs(np.subtract(error_t, error_t_prev)) < e:\n",
        "                yield W, C, t, e\n",
        "                break\n",
        "\n",
        "X = abs(np.random.rand(500, 1000))\n",
        "k = 10\n",
        "lambda_ = 0.1\n",
        "epsilon = [0.01, 0.001, 0.0001]\n",
        "\n",
        "gen = RegNMF(X, k, lambda_, epsilon)\n",
        "for W, C, t, e in gen:\n",
        "    print(\"W : \", W)\n",
        "    print(\"C : \", C)\n",
        "    print(\"Run for {} iterations and epsilon {} \\n\".format(t, e))\n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfvdytHe75df",
        "colab_type": "text"
      },
      "source": [
        "Συμφωνα με τον αλγόριθμο παρακολουθούμε το σφάλμα ανακατασκευής και αν η τιμή του ανάμεσα\n",
        "σε δυο διαδοχικές επαναλήψεις είναι μικρότερη απο ενα κατώφλι ε τερματίζουμε τον αλγόριθμο\n",
        "Το πείραμα δείχνει οτι σε περίπτωση που επιλέξουμε πολυ μικρό ε χρειάζονται πολύ περισσότερες\n",
        "επαναλλήψεις (40+ στο παράδειγμα) προκειμένου να συγκλίνει και να τερματίσει ο αλγόριθμος, σε σχέση με μεγαλύτερες τιμές του ε"
      ]
    }
  ]
}